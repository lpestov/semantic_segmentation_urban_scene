{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0aa212f5",
      "metadata": {},
      "source": [
        "# DINOv2-Small evaluation on ACDC\n",
        "\n",
        "This notebook evaluates a trained DINOv2-Small checkpoint on the ACDC validation split.\n",
        "It includes:\n",
        "- environment setup for Colab,\n",
        "- dataset unpacking,\n",
        "- checkpoint loading,\n",
        "- metrics + plots,\n",
        "- ClearML logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7efeab9",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q albumentations torchmetrics pytorch-lightning clearml python-dotenv gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e5800cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchmetrics.classification import MulticlassJaccardIndex\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from clearml import Task\n",
        "from google.colab import drive, userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42db2a6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    \"project_name\": \"Segmentation_Urban_Scene_CourseWork\",\n",
        "    \"task_name\": \"DinoV2_Small_Eval_ACDC_Val\",\n",
        "    \"drive_root\": \"/content/drive/MyDrive\",\n",
        "    \"weights_rel_path\": \"weights/dinov2-small-cityscapes-epoch=48-val_miou=0.6292.ckpt\",\n",
        "    \"acdc_zips\": [\n",
        "        \"gt_trainval.zip\",\n",
        "        \"rgb_anon_trainvaltest.zip\",\n",
        "    ],\n",
        "    \"data_dir\": \"/content/data/acdc\",\n",
        "    \"split\": \"val\",\n",
        "    \"conditions\": [\"fog\", \"night\", \"rain\", \"snow\"],\n",
        "    \"model_name\": \"dinov2_vits14\",\n",
        "    \"num_classes\": 19,\n",
        "    \"image_size\": (518, 1022),\n",
        "    \"batch_size\": 8,\n",
        "    \"num_workers\": 2,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "}\n",
        "\n",
        "CITYSCAPES_CLASSES = [\n",
        "    \"road\", \"sidewalk\", \"building\", \"wall\", \"fence\", \"pole\", \"traffic light\",\n",
        "    \"traffic sign\", \"vegetation\", \"terrain\", \"sky\", \"person\", \"rider\", \"car\",\n",
        "    \"truck\", \"bus\", \"train\", \"motorcycle\", \"bicycle\"\n",
        "]\n",
        "\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "os.environ[\"CLEARML_API_ACCESS_KEY\"] = userdata.get(\"CLEARML_API_ACCESS_KEY\")\n",
        "os.environ[\"CLEARML_API_SECRET_KEY\"] = userdata.get(\"CLEARML_API_SECRET_KEY\")\n",
        "\n",
        "task = Task.init(\n",
        "    project_name=CONFIG[\"project_name\"],\n",
        "    task_name=CONFIG[\"task_name\"],\n",
        "    output_uri=False,\n",
        ")\n",
        "task.connect(CONFIG)\n",
        "\n",
        "print(f\"Using device: {CONFIG['device']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f104965",
      "metadata": {},
      "outputs": [],
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "Path(CONFIG[\"data_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for zip_name in CONFIG[\"acdc_zips\"]:\n",
        "    zip_path = Path(CONFIG[\"drive_root\"]) / zip_name\n",
        "    if not zip_path.exists():\n",
        "        raise FileNotFoundError(f\"Zip file not found: {zip_path}\")\n",
        "\n",
        "    print(f\"Unpacking {zip_path} -> {CONFIG['data_dir']}\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(CONFIG[\"data_dir\"])\n",
        "\n",
        "weights_path = Path(CONFIG[\"drive_root\"]) / CONFIG[\"weights_rel_path\"]\n",
        "if not weights_path.exists():\n",
        "    raise FileNotFoundError(f\"Checkpoint not found: {weights_path}\")\n",
        "\n",
        "print(f\"Checkpoint path: {weights_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3f5f967",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ACDCDataset(Dataset):\n",
        "    def __init__(self, root_dir, split=\"val\", conditions=None, augmentation=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.split = split\n",
        "        self.conditions = conditions or [\"fog\", \"night\", \"rain\", \"snow\"]\n",
        "        self.augmentation = augmentation\n",
        "        self.items = []\n",
        "\n",
        "        for condition in self.conditions:\n",
        "            rgb_root = self.root_dir / \"rgb_anon\" / condition / split\n",
        "            gt_root = self.root_dir / \"gt\" / condition / split\n",
        "            if not rgb_root.exists() or not gt_root.exists():\n",
        "                print(f\"Skip missing split folder: {condition}/{split}\")\n",
        "                continue\n",
        "\n",
        "            for image_path in sorted(rgb_root.rglob(\"*_rgb_anon.png\")):\n",
        "                rel = image_path.relative_to(rgb_root)\n",
        "                mask_name = image_path.name.replace(\"_rgb_anon.png\", \"_gt_labelTrainIds.png\")\n",
        "                mask_path = gt_root / rel.parent / mask_name\n",
        "                if mask_path.exists():\n",
        "                    self.items.append(\n",
        "                        {\n",
        "                            \"image\": str(image_path),\n",
        "                            \"mask\": str(mask_path),\n",
        "                            \"condition\": condition,\n",
        "                            \"stem\": image_path.stem,\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "        if len(self.items) == 0:\n",
        "            raise RuntimeError(\"No ACDC image/mask pairs found. Check dataset path and split.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.items[idx]\n",
        "        image = cv2.imread(sample[\"image\"])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(sample[\"mask\"], 0)\n",
        "\n",
        "        if self.augmentation is not None:\n",
        "            transformed = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = transformed[\"image\"], transformed[\"mask\"]\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"mask\": mask.long(),\n",
        "            \"condition\": sample[\"condition\"],\n",
        "            \"stem\": sample[\"stem\"],\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43886c8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "valid_transform = A.Compose([\n",
        "    A.Resize(height=CONFIG[\"image_size\"][0], width=CONFIG[\"image_size\"][1]),\n",
        "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "dataset = ACDCDataset(\n",
        "    root_dir=CONFIG[\"data_dir\"],\n",
        "    split=CONFIG[\"split\"],\n",
        "    conditions=CONFIG[\"conditions\"],\n",
        "    augmentation=valid_transform,\n",
        ")\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG[\"num_workers\"],\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "print(f\"ACDC samples: {len(dataset)}\")\n",
        "print(pd.DataFrame(dataset.items)[\"condition\"].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f3b6526",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearSegmentationHead(nn.Module):\n",
        "    def __init__(self, embed_dim=384, num_classes=19, patch_size=14):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.patch_size = patch_size\n",
        "        self.linear = nn.Conv2d(embed_dim, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, h, w):\n",
        "        bsz, n_tokens, channels = x.shape\n",
        "        patch_h = h // self.patch_size\n",
        "        patch_w = w // self.patch_size\n",
        "        x = x.reshape(bsz, patch_h, patch_w, channels).permute(0, 3, 1, 2)\n",
        "        x = self.linear(x)\n",
        "        x = F.interpolate(x, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DinoV2SegmentationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"dinov2_vits14\", num_classes=19):\n",
        "        super().__init__()\n",
        "        self.backbone = torch.hub.load(\"facebookresearch/dinov2\", model_name)\n",
        "        self.embed_dim = 384\n",
        "        self.patch_size = 14\n",
        "        self.segmentation_head = LinearSegmentationHead(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_classes=num_classes,\n",
        "            patch_size=self.patch_size,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, h, w = x.shape\n",
        "        features = self.backbone.forward_features(x)\n",
        "        patch_features = features[\"x_norm_patchtokens\"]\n",
        "        logits = self.segmentation_head(patch_features, h, w)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def load_lightning_checkpoint(model, ckpt_path, device):\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    state_dict = ckpt.get(\"state_dict\", ckpt)\n",
        "\n",
        "    model_state = {}\n",
        "    for key, value in state_dict.items():\n",
        "        if key.startswith(\"backbone.\") or key.startswith(\"segmentation_head.\"):\n",
        "            model_state[key] = value\n",
        "\n",
        "    missing, unexpected = model.load_state_dict(model_state, strict=False)\n",
        "    print(f\"Missing keys: {len(missing)}\")\n",
        "    print(f\"Unexpected keys: {len(unexpected)}\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "model = DinoV2SegmentationModel(\n",
        "    model_name=CONFIG[\"model_name\"],\n",
        "    num_classes=CONFIG[\"num_classes\"],\n",
        ")\n",
        "model = load_lightning_checkpoint(model, str(weights_path), CONFIG[\"device\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2b50233",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, num_classes=19, device=\"cpu\"):\n",
        "    metric_all = MulticlassJaccardIndex(num_classes=num_classes, average=\"macro\", ignore_index=255).to(device)\n",
        "    metric_per_class = MulticlassJaccardIndex(num_classes=num_classes, average=\"none\", ignore_index=255).to(device)\n",
        "\n",
        "    condition_metrics = {\n",
        "        cond: MulticlassJaccardIndex(num_classes=num_classes, average=\"macro\", ignore_index=255).to(device)\n",
        "        for cond in CONFIG[\"conditions\"]\n",
        "    }\n",
        "\n",
        "    total_pixels = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    cache = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        images = batch[\"image\"].to(device)\n",
        "        masks = batch[\"mask\"].to(device)\n",
        "        conditions = batch[\"condition\"]\n",
        "\n",
        "        logits = model(images)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        metric_all.update(preds, masks)\n",
        "        metric_per_class.update(preds, masks)\n",
        "\n",
        "        valid = masks != 255\n",
        "        total_correct += ((preds == masks) & valid).sum().item()\n",
        "        total_pixels += valid.sum().item()\n",
        "\n",
        "        for i, cond in enumerate(conditions):\n",
        "            condition_metrics[cond].update(preds[i:i+1], masks[i:i+1])\n",
        "\n",
        "        cache.append(\n",
        "            {\n",
        "                \"images\": images.detach().cpu(),\n",
        "                \"masks\": masks.detach().cpu(),\n",
        "                \"preds\": preds.detach().cpu(),\n",
        "                \"conditions\": list(conditions),\n",
        "                \"stems\": list(batch[\"stem\"]),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    overall_miou = metric_all.compute().item()\n",
        "    class_iou = metric_per_class.compute().detach().cpu().numpy()\n",
        "    pixel_acc = total_correct / max(total_pixels, 1)\n",
        "\n",
        "    condition_miou = {\n",
        "        cond: metric.compute().item()\n",
        "        for cond, metric in condition_metrics.items()\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"overall_miou\": overall_miou,\n",
        "        \"pixel_accuracy\": pixel_acc,\n",
        "        \"class_iou\": class_iou,\n",
        "        \"condition_miou\": condition_miou,\n",
        "        \"cache\": cache,\n",
        "    }\n",
        "\n",
        "\n",
        "results = evaluate(model, loader, num_classes=CONFIG[\"num_classes\"], device=CONFIG[\"device\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b711a9d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_df = pd.DataFrame(\n",
        "    {\n",
        "        \"metric\": [\"mIoU\", \"Pixel Accuracy\"],\n",
        "        \"value\": [results[\"overall_miou\"], results[\"pixel_accuracy\"]],\n",
        "    }\n",
        ")\n",
        "\n",
        "condition_df = pd.DataFrame(\n",
        "    {\n",
        "        \"condition\": list(results[\"condition_miou\"].keys()),\n",
        "        \"mIoU\": list(results[\"condition_miou\"].values()),\n",
        "    }\n",
        ").sort_values(\"condition\")\n",
        "\n",
        "class_df = pd.DataFrame(\n",
        "    {\n",
        "        \"class_id\": list(range(CONFIG[\"num_classes\"])),\n",
        "        \"class_name\": CITYSCAPES_CLASSES,\n",
        "        \"IoU\": results[\"class_iou\"],\n",
        "    }\n",
        ").sort_values(\"IoU\", ascending=False)\n",
        "\n",
        "print(\"Overall metrics\")\n",
        "display(summary_df)\n",
        "print(\"\\nPer-condition mIoU\")\n",
        "display(condition_df)\n",
        "print(\"\\nPer-class IoU\")\n",
        "display(class_df)\n",
        "\n",
        "logger = task.get_logger()\n",
        "logger.report_table(\"evaluation\", \"overall\", iteration=0, table_plot=summary_df)\n",
        "logger.report_table(\"evaluation\", \"per_condition\", iteration=0, table_plot=condition_df)\n",
        "logger.report_table(\"evaluation\", \"per_class\", iteration=0, table_plot=class_df)\n",
        "logger.report_scalar(\"evaluation\", \"mIoU\", results[\"overall_miou\"], iteration=0)\n",
        "logger.report_scalar(\"evaluation\", \"pixel_accuracy\", results[\"pixel_accuracy\"], iteration=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a327fc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig1, ax1 = plt.subplots(figsize=(7, 4))\n",
        "ax1.bar(condition_df[\"condition\"], condition_df[\"mIoU\"])\n",
        "ax1.set_ylim(0, 1)\n",
        "ax1.set_ylabel(\"mIoU\")\n",
        "ax1.set_title(\"DINOv2-Small on ACDC val: per-condition mIoU\")\n",
        "ax1.grid(axis=\"y\", alpha=0.25)\n",
        "fig1.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "logger.report_matplotlib_figure(\n",
        "    title=\"plots\",\n",
        "    series=\"per_condition_miou\",\n",
        "    iteration=0,\n",
        "    figure=fig1,\n",
        ")\n",
        "\n",
        "plot_df = class_df.sort_values(\"IoU\", ascending=False)\n",
        "fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
        "ax2.bar(plot_df[\"class_name\"], plot_df[\"IoU\"])\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.set_ylabel(\"IoU\")\n",
        "ax2.set_title(\"Per-class IoU (sorted)\")\n",
        "ax2.grid(axis=\"y\", alpha=0.25)\n",
        "ax2.tick_params(axis=\"x\", rotation=70)\n",
        "fig2.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "logger.report_matplotlib_figure(\n",
        "    title=\"plots\",\n",
        "    series=\"per_class_iou\",\n",
        "    iteration=0,\n",
        "    figure=fig2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "320eeda9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def denormalize_for_display(img_tensor):\n",
        "    img = img_tensor.permute(1, 2, 0).numpy().astype(np.float32)\n",
        "    mean = np.array(IMAGENET_MEAN, dtype=np.float32)\n",
        "    std = np.array(IMAGENET_STD, dtype=np.float32)\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return img\n",
        "\n",
        "\n",
        "def _collect_condition_samples_from_cache(cached_batches, conditions, samples_per_condition=3):\n",
        "    collected = {cond: [] for cond in conditions}\n",
        "    for batch in cached_batches:\n",
        "        for i, cond in enumerate(batch[\"conditions\"]):\n",
        "            if cond in collected and len(collected[cond]) < samples_per_condition:\n",
        "                collected[cond].append(\n",
        "                    {\n",
        "                        \"image\": batch[\"images\"][i],\n",
        "                        \"mask\": batch[\"masks\"][i],\n",
        "                        \"pred\": batch[\"preds\"][i],\n",
        "                    }\n",
        "                )\n",
        "        if all(len(collected[c]) >= samples_per_condition for c in conditions):\n",
        "            break\n",
        "    return collected\n",
        "\n",
        "\n",
        "def show_predictions_by_condition(cached_batches, condition_miou, conditions, samples_per_condition=3):\n",
        "    samples = _collect_condition_samples_from_cache(cached_batches, conditions, samples_per_condition)\n",
        "\n",
        "    rows = len(conditions) * samples_per_condition\n",
        "    fig, axes = plt.subplots(rows, 3, figsize=(14, rows * 3.2))\n",
        "    if rows == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    row_idx = 0\n",
        "    for cond in conditions:\n",
        "        cond_score = condition_miou.get(cond, float(\"nan\"))\n",
        "        for s_idx in range(samples_per_condition):\n",
        "            if s_idx < len(samples[cond]):\n",
        "                sample = samples[cond][s_idx]\n",
        "                image = denormalize_for_display(sample[\"image\"])\n",
        "                gt = sample[\"mask\"].numpy()\n",
        "                pred = sample[\"pred\"].numpy()\n",
        "\n",
        "                axes[row_idx, 0].imshow(image)\n",
        "                axes[row_idx, 0].set_title(f\"{cond} #{s_idx + 1} | image\")\n",
        "                axes[row_idx, 0].axis(\"off\")\n",
        "\n",
        "                axes[row_idx, 1].imshow(gt, vmin=0, vmax=18)\n",
        "                axes[row_idx, 1].set_title(f\"GT | {cond} mIoU={cond_score:.4f}\")\n",
        "                axes[row_idx, 1].axis(\"off\")\n",
        "\n",
        "                axes[row_idx, 2].imshow(pred, vmin=0, vmax=18)\n",
        "                axes[row_idx, 2].set_title(\"Prediction\")\n",
        "                axes[row_idx, 2].axis(\"off\")\n",
        "            else:\n",
        "                axes[row_idx, 0].axis(\"off\")\n",
        "                axes[row_idx, 1].axis(\"off\")\n",
        "                axes[row_idx, 2].axis(\"off\")\n",
        "            row_idx += 1\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def _predict_ref_single_image(image_path, model, resize_hw, device):\n",
        "    image = cv2.imread(str(image_path))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    transformed = A.Compose([\n",
        "        A.Resize(height=resize_hw[0], width=resize_hw[1]),\n",
        "        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "        ToTensorV2(),\n",
        "    ])(image=image)\n",
        "\n",
        "    image_resized = A.Resize(height=resize_hw[0], width=resize_hw[1])(image=image)[\"image\"]\n",
        "    image_tensor = transformed[\"image\"].unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(image_tensor)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        pred = torch.argmax(probs, dim=1).squeeze(0).cpu().numpy()\n",
        "        max_prob = probs.max(dim=1).values.squeeze(0).cpu().numpy()\n",
        "\n",
        "    mean_conf = float(max_prob.mean())\n",
        "    entropy = float((-probs * torch.log(probs + 1e-8)).sum(dim=1).mean().item())\n",
        "\n",
        "    return {\n",
        "        \"image\": image_resized,\n",
        "        \"pred\": pred,\n",
        "        \"mean_conf\": mean_conf,\n",
        "        \"mean_entropy\": entropy,\n",
        "    }\n",
        "\n",
        "\n",
        "def show_ref_predictions_by_condition(root_dir, split_ref, conditions, model, resize_hw, device, samples_per_condition=3):\n",
        "    collected = {cond: [] for cond in conditions}\n",
        "\n",
        "    for cond in conditions:\n",
        "        ref_root = Path(root_dir) / \"rgb_anon\" / cond / split_ref\n",
        "        if not ref_root.exists():\n",
        "            continue\n",
        "\n",
        "        files = sorted(ref_root.rglob(\"*_rgb_ref_anon.png\"))\n",
        "        for fp in files[:samples_per_condition]:\n",
        "            pred_item = _predict_ref_single_image(\n",
        "                image_path=fp,\n",
        "                model=model,\n",
        "                resize_hw=resize_hw,\n",
        "                device=device,\n",
        "            )\n",
        "            collected[cond].append(pred_item)\n",
        "\n",
        "    rows = len(conditions) * samples_per_condition\n",
        "    fig, axes = plt.subplots(rows, 2, figsize=(12, rows * 3.2))\n",
        "    if rows == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    row_idx = 0\n",
        "    metrics_rows = []\n",
        "    for cond in conditions:\n",
        "        conf_vals = []\n",
        "        ent_vals = []\n",
        "        for s_idx in range(samples_per_condition):\n",
        "            if s_idx < len(collected[cond]):\n",
        "                item = collected[cond][s_idx]\n",
        "                conf_vals.append(item[\"mean_conf\"])\n",
        "                ent_vals.append(item[\"mean_entropy\"])\n",
        "\n",
        "                axes[row_idx, 0].imshow(item[\"image\"])\n",
        "                axes[row_idx, 0].set_title(f\"{cond} ref #{s_idx + 1} | image\")\n",
        "                axes[row_idx, 0].axis(\"off\")\n",
        "\n",
        "                axes[row_idx, 1].imshow(item[\"pred\"], vmin=0, vmax=18)\n",
        "                axes[row_idx, 1].set_title(\n",
        "                    f\"Prediction | conf={item['mean_conf']:.3f}, H={item['mean_entropy']:.3f}\"\n",
        "                )\n",
        "                axes[row_idx, 1].axis(\"off\")\n",
        "            else:\n",
        "                axes[row_idx, 0].axis(\"off\")\n",
        "                axes[row_idx, 1].axis(\"off\")\n",
        "            row_idx += 1\n",
        "\n",
        "        if len(conf_vals) > 0:\n",
        "            metrics_rows.append(\n",
        "                {\n",
        "                    \"condition\": cond,\n",
        "                    \"mean_confidence\": float(np.mean(conf_vals)),\n",
        "                    \"mean_entropy\": float(np.mean(ent_vals)),\n",
        "                    \"num_samples\": len(conf_vals),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics_rows)\n",
        "    return fig, metrics_df\n",
        "\n",
        "\n",
        "fig3 = show_predictions_by_condition(\n",
        "    cached_batches=results[\"cache\"],\n",
        "    condition_miou=results[\"condition_miou\"],\n",
        "    conditions=CONFIG[\"conditions\"],\n",
        "    samples_per_condition=3,\n",
        ")\n",
        "logger.report_matplotlib_figure(\n",
        "    title=\"plots\",\n",
        "    series=\"qualitative_predictions_by_condition\",\n",
        "    iteration=0,\n",
        "    figure=fig3,\n",
        ")\n",
        "\n",
        "fig4, ref_metrics_df = show_ref_predictions_by_condition(\n",
        "    root_dir=CONFIG[\"data_dir\"],\n",
        "    split_ref=f\"{CONFIG['split']}_ref\",\n",
        "    conditions=CONFIG[\"conditions\"],\n",
        "    model=model,\n",
        "    resize_hw=CONFIG[\"image_size\"],\n",
        "    device=CONFIG[\"device\"],\n",
        "    samples_per_condition=3,\n",
        ")\n",
        "logger.report_matplotlib_figure(\n",
        "    title=\"plots\",\n",
        "    series=\"qualitative_predictions_ref_by_condition\",\n",
        "    iteration=0,\n",
        "    figure=fig4,\n",
        ")\n",
        "\n",
        "if len(ref_metrics_df) > 0:\n",
        "    print(\"Ref split proxy metrics (no GT-based IoU):\")\n",
        "    display(ref_metrics_df)\n",
        "    logger.report_table(\"evaluation\", \"ref_proxy_metrics\", iteration=0, table_plot=ref_metrics_df)\n",
        "    for _, row in ref_metrics_df.iterrows():\n",
        "        logger.report_scalar(\"ref_proxy\", f\"confidence_{row['condition']}\", row[\"mean_confidence\"], iteration=0)\n",
        "        logger.report_scalar(\"ref_proxy\", f\"entropy_{row['condition']}\", row[\"mean_entropy\"], iteration=0)\n",
        "else:\n",
        "    print(\"No ref images found for selected split.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b67d834",
      "metadata": {},
      "outputs": [],
      "source": [
        "task.close()\n",
        "print(\"ClearML task closed.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
